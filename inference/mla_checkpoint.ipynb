{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803326dd-a5da-4a24-9122-ff75034f5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoTokenizer\n",
    "from safetensors.torch import load_model\n",
    "\n",
    "from model import Transformer, ModelArgs\n",
    "from generate import sample, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dad36-ff28-454a-9ff8-ecb85306eb21",
   "metadata": {},
   "source": [
    "## Prepare model and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f3c651-a949-485e-97a2-ef7cf2d301f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(max_batch_size=8, max_seq_len=16384, dtype='bf16', vocab_size=102400, dim=2048, inter_dim=10944, moe_inter_dim=1408, n_layers=27, n_dense_layers=1, n_heads=16, n_routed_experts=64, n_shared_experts=2, n_activated_experts=6, n_expert_groups=1, n_limited_groups=1, score_func='softmax', route_scale=1.0, q_lora_rank=0, kv_lora_rank=512, qk_nope_head_dim=128, qk_rope_head_dim=64, v_head_dim=128, original_seq_len=4096, rope_theta=10000.0, rope_factor=40, beta_fast=32, beta_slow=1, mscale=0.707)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(set(), [])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = \"/home/DeepSeek-V2-Lite-Chat_converted\"\n",
    "config = \"configs/config_16B.json\"\n",
    "input_file = \"input_file.txt\"\n",
    "max_new_tokens: int = 200\n",
    "temperature: float = 0.2\n",
    "\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "torch.set_num_threads(8)\n",
    "torch.manual_seed(965)\n",
    "\n",
    "with open(config) as f:\n",
    "    args = ModelArgs(**json.load(f))\n",
    "print(args)\n",
    "\n",
    "with torch.device(\"cuda\"):\n",
    "    model = Transformer(args)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "tokenizer.decode(generate(model, [tokenizer.encode(\"DeepSeek\")], 2, -1, 1.)[0])\n",
    "\n",
    "rank, world_size = 0, 1  # single-device\n",
    "load_model(model, os.path.join(ckpt_path, f\"model{rank}-mp{world_size}.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7620ac2-034e-4140-9879-b3ce74624c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a9ba73-ab36-41bc-a91a-407b94204d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[100000, 5726, 25, 37727, 0, 185, 185, 77398, 25],\n",
       " [100000, 5726, 25, 1724, 418, 340, 30, 185, 185, 77398, 25],\n",
       " [100000, 5726, 25, 7566, 2653, 13, 185, 185, 77398, 25]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(input_file) as f:\n",
    "    prompts = [line.strip() for line in f.readlines()]\n",
    "assert len(prompts) <= args.max_batch_size, f\"Number of prompts exceeds maximum batch size ({args.max_batch_size})\"\n",
    "\n",
    "prompt_tokens = [tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True) for prompt in prompts]\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5cf7ae4-6706-4b3e-8cc6-fae819793959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.82 s, sys: 42.7 ms, total: 3.86 s\n",
      "Wall time: 3.86 s\n",
      "Prompt: Hello!\n",
      "Completion:  Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.\n",
      "Prompt: How are you?\n",
      "Completion:  As an AI, I do not have feelings, but I am functioning properly and ready to assist you with any questions or tasks you have.\n",
      "Prompt: Good night.\n",
      "Completion:  Good night! Have a great rest and pleasant dreams.\n"
     ]
    }
   ],
   "source": [
    "%time completion_tokens = generate(model, prompt_tokens, max_new_tokens, tokenizer.eos_token_id, temperature)\n",
    "\n",
    "completions = tokenizer.batch_decode(completion_tokens, skip_special_tokens=True)\n",
    "for prompt, completion in zip(prompts, completions):\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Completion:\", completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679258c8-0d5b-41b5-b5f1-d2f1f21adb99",
   "metadata": {},
   "source": [
    "## Checkpoints MLA input output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506c2948-4bde-4618-800f-c491b5bab06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mla_layer = model.layers[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2b85d8-5e63-4572-a66c-4b80a5b8476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1147213867929261"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mla_layer.softmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38e94f1-ed6c-4b30-bc09-6aaa00595d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 'output_ckpt', 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mla_layer.save_ckpt, mla_layer.ckpt_dir, mla_layer.ckpt_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22898783-2f2d-4f78-9361-6b6267554964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir output_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7714e062-4dde-4883-adae-0417d6aa5a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving ckpt to:  output_ckpt/mla_ckpt_0.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_1.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_2.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_3.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_4.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_5.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_6.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_7.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_8.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_9.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_10.safetensors\n",
      "saving ckpt to:  output_ckpt/mla_ckpt_11.safetensors\n",
      "Prompt: Hello!\n",
      "Completion:  Hello! How can I help you today? If\n",
      "Prompt: How are you?\n",
      "Completion:  As an AI, I do not have feelings,\n",
      "Prompt: Good night.\n",
      "Completion:  Good night! Have a great rest and pleasant dreams\n"
     ]
    }
   ],
   "source": [
    "mla_layer.save_ckpt = True\n",
    "mla_layer.ckpt_iter = 0\n",
    "\n",
    "completion_tokens = generate(\n",
    "    model,\n",
    "    prompt_tokens,\n",
    "    10, # max_new_tokens,\n",
    "    tokenizer.eos_token_id,\n",
    "    temperature\n",
    ")\n",
    "\n",
    "completions = tokenizer.batch_decode(completion_tokens, skip_special_tokens=True)\n",
    "for prompt, completion in zip(prompts, completions):\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Completion:\", completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0859fa60-6e8e-4111-8514-e278a7e8ddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.7M\n",
      "-rw-r--r-- 1 root root 957K Mar 29 08:31 mla_ckpt_0.safetensors\n",
      "-rw-r--r-- 1 root root 138K Mar 29 08:31 mla_ckpt_1.safetensors\n",
      "-rw-r--r-- 1 root root 169K Mar 29 08:31 mla_ckpt_10.safetensors\n",
      "-rw-r--r-- 1 root root 172K Mar 29 08:31 mla_ckpt_11.safetensors\n",
      "-rw-r--r-- 1 root root 141K Mar 29 08:31 mla_ckpt_2.safetensors\n",
      "-rw-r--r-- 1 root root 145K Mar 29 08:31 mla_ckpt_3.safetensors\n",
      "-rw-r--r-- 1 root root 148K Mar 29 08:31 mla_ckpt_4.safetensors\n",
      "-rw-r--r-- 1 root root 152K Mar 29 08:31 mla_ckpt_5.safetensors\n",
      "-rw-r--r-- 1 root root 155K Mar 29 08:31 mla_ckpt_6.safetensors\n",
      "-rw-r--r-- 1 root root 159K Mar 29 08:31 mla_ckpt_7.safetensors\n",
      "-rw-r--r-- 1 root root 162K Mar 29 08:31 mla_ckpt_8.safetensors\n",
      "-rw-r--r-- 1 root root 165K Mar 29 08:31 mla_ckpt_9.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls -lh output_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37de78-090c-4c29-9fd7-a6ba46309422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
