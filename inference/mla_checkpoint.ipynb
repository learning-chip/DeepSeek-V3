{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803326dd-a5da-4a24-9122-ff75034f5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from transformers import AutoTokenizer\n",
    "from safetensors.torch import load_model\n",
    "\n",
    "from model import Transformer, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a0292b-25ad-4406-9f34-f64235519bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import sample, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff9148c-86e5-43ea-b0df-2c6f9a66cb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(max_batch_size=8, max_seq_len=16384, dtype='bf16', vocab_size=102400, dim=2048, inter_dim=10944, moe_inter_dim=1408, n_layers=27, n_dense_layers=1, n_heads=16, n_routed_experts=64, n_shared_experts=2, n_activated_experts=6, n_expert_groups=1, n_limited_groups=1, score_func='softmax', route_scale=1.0, q_lora_rank=0, kv_lora_rank=512, qk_nope_head_dim=128, qk_rope_head_dim=64, v_head_dim=128, original_seq_len=4096, rope_theta=10000.0, rope_factor=40, beta_fast=32, beta_slow=1, mscale=0.707)\n",
      "Prompt: Hello!\n",
      "Completion:  Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.\n",
      "Prompt: How are you?\n",
      "Completion:  As an AI, I do not have feelings, but I am functioning properly and ready to assist you with any questions or tasks you have.\n",
      "Prompt: Good night.\n",
      "Completion:  Good night! Have a great rest and pleasant dreams.\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"/home/DeepSeek-V2-Lite-Chat_converted\"\n",
    "config = \"configs/config_16B.json\"\n",
    "input_file = \"input_file.txt\"\n",
    "max_new_tokens: int = 200\n",
    "temperature: float = 0.2\n",
    "\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "torch.set_num_threads(8)\n",
    "torch.manual_seed(965)\n",
    "\n",
    "with open(config) as f:\n",
    "    args = ModelArgs(**json.load(f))\n",
    "print(args)\n",
    "\n",
    "with torch.device(\"cuda\"):\n",
    "    model = Transformer(args)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n",
    "tokenizer.decode(generate(model, [tokenizer.encode(\"DeepSeek\")], 2, -1, 1.)[0])\n",
    "\n",
    "rank, world_size = 0, 1  # single-device\n",
    "load_model(model, os.path.join(ckpt_path, f\"model{rank}-mp{world_size}.safetensors\"))\n",
    "\n",
    "with open(input_file) as f:\n",
    "    prompts = [line.strip() for line in f.readlines()]\n",
    "assert len(prompts) <= args.max_batch_size, f\"Number of prompts exceeds maximum batch size ({args.max_batch_size})\"\n",
    "\n",
    "prompt_tokens = [tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True) for prompt in prompts]\n",
    "completion_tokens = generate(model, prompt_tokens, max_new_tokens, tokenizer.eos_token_id, temperature)\n",
    "completions = tokenizer.batch_decode(completion_tokens, skip_special_tokens=True)\n",
    "\n",
    "for prompt, completion in zip(prompts, completions):\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Completion:\", completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf7ae4-6706-4b3e-8cc6-fae819793959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
